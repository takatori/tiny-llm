{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc368140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e58f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 768,\n",
    "    \"context_length\": 1024,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b4df02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f212b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5ebb312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) \n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a94d5f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0af7ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Output: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean after normalization: tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance after normalization: tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized Output:\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean after normalization:\", mean)\n",
    "print(\"Variance after normalization:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c942779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c69c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # 正規化時に0除算を防ぐために分散に加算される小さな定数\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # 訓練可能なパラメータ\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # 訓練可能なパラメータ\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # 埋め込み次元nが非常に大きいため、分散の計算で不偏推定を使用しない\n",
    "        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * normalized_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dcf5f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "LayerNorm Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"LayerNorm Mean:\\n\", mean)\n",
    "print(\"LayerNorm Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8ae66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd1214",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (y, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m([y_gelu, y_relu], [\u001b[33m\"\u001b[39m\u001b[33mGELU\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mReLU\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[32m1\u001b[39m):\n\u001b[32m      9\u001b[39m     plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, i)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Activatiodn Function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:483\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    480\u001b[39m         kw[prop_name] = val\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     x = \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     y = _check_1d(xy[\u001b[32m1\u001b[39m])\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/cbook.py:1361\u001b[39m, in \u001b[36m_check_1d\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1359\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[39;00m\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# Unpack in case of e.g. Pandas or xarray object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m x = \u001b[43m_unpack_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[32m   1363\u001b[39m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[32m   1364\u001b[39m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1367\u001b[39m         \u001b[38;5;28mlen\u001b[39m(x.shape) < \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/matplotlib/cbook.py:2371\u001b[39m, in \u001b[36m_unpack_to_numpy\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xtmp\n\u001b[32m   2366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_array(x) \u001b[38;5;129;01mor\u001b[39;00m _is_jax_array(x) \u001b[38;5;129;01mor\u001b[39;00m _is_tensorflow_array(x):\n\u001b[32m   2367\u001b[39m     \u001b[38;5;66;03m# using np.asarray() instead of explicitly __array__(), as the latter is\u001b[39;00m\n\u001b[32m   2368\u001b[39m     \u001b[38;5;66;03m# only _one_ of many methods, and it's the last resort, see also\u001b[39;00m\n\u001b[32m   2369\u001b[39m     \u001b[38;5;66;03m# https://numpy.org/devdocs/user/basics.interoperability.html#using-arbitrary-objects-in-numpy\u001b[39;00m\n\u001b[32m   2370\u001b[39m     \u001b[38;5;66;03m# therefore, let arrays do better if they can\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m     xtmp = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# In case np.asarray method does not return a numpy array in future\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xtmp, np.ndarray):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/tiny-llm/.venv/lib/python3.13/site-packages/torch/_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEYCAYAAAAZNO4sAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFbpJREFUeJzt3X9MVff9x/E3PwQ0LdiOCcqwtHb2x6zQgjC0pnGhJdHY+kczpkYoqTqnMx1krVAt1LqKc9aQTKyp1dk/ZqVttGmKwbW0pHHSkGJN7KY2lrawpiCsExi2oHC++Xy+ucyLF8ulvOXe4/ORnMg5fD73no/g637u57zvMcRxHEcAAKMudPQfEgBgELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsAAQKAH7wQcfyMKFC2XKlCkSEhIib7755vf2qa2tlfvuu08iIyPl9ttvl3379o30fAHAvQHb3d0tycnJUlFRMaz2n3/+uSxYsEDmzZsnJ06ckN/97neyfPlyOXLkyEjOFwCCRsgPudmLmcEeOnRIFi1aNGSbdevWSVVVlXzyyScDx371q1/J+fPnpbq6eqRPDQABL1z7Cerq6iQrK8vrWHZ2tp3JDqWnp8duHv39/fLNN9/Ij370IxvqADDazFyzq6vLLn+GhoYGR8C2tLRIXFyc1zGz39nZKd9++62MHz/+ij5lZWWyceNG7VMDgCs0NzfLT37yEwmKgB2J4uJiKSwsHNjv6OiQqVOn2oFHR0eP6bkBcKfOzk5JTEyUG2+8cdQeUz1g4+PjpbW11euY2TdB6Wv2aphqA7MNZvoQsAA0jeYypHodbGZmptTU1Hgde+edd+xxAHAzvwP2v//9ry23MpunDMt83dTUNPD2Pjc3d6D9qlWrpLGxUZ566ik5ffq07Ny5U1577TUpKCgYzXEAQPAH7EcffST33nuv3QyzVmq+Likpsftff/31QNgat956qy3TMrNWUz/7wgsvyMsvv2wrCQDAzX5QHey1XHyOiYmxF7tYgwUQLDnDvQgAQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWAAIpICtqKiQpKQkiYqKkoyMDKmvr79q+/Lycrnjjjtk/PjxkpiYKAUFBfLdd9+N9JwBwJ0BW1lZKYWFhVJaWirHjx+X5ORkyc7OlnPnzvlsv3//fikqKrLtT506JXv27LGP8fTTT4/G+QOAewJ2+/btsmLFCsnPz5e7775bdu3aJRMmTJC9e/f6bH/s2DGZM2eOLFmyxM56H3roIVm8ePH3znoB4LoK2N7eXmloaJCsrKz/PUBoqN2vq6vz2Wf27Nm2jydQGxsb5fDhwzJ//vwhn6enp0c6Ozu9NgAINuH+NG5vb5e+vj6Ji4vzOm72T58+7bOPmbmafvfff784jiOXLl2SVatWXXWJoKysTDZu3OjPqQHA9VdFUFtbK5s3b5adO3faNduDBw9KVVWVbNq0acg+xcXF0tHRMbA1NzdrnyYAjO0MNjY2VsLCwqS1tdXruNmPj4/32eeZZ56RZcuWyfLly+3+PffcI93d3bJy5UpZv369XWIYLDIy0m4AcN3MYCMiIiQ1NVVqamoGjvX399v9zMxMn30uXLhwRYiakDbMkgEAuJVfM1jDlGjl5eVJWlqapKen2xpXMyM1VQVGbm6uJCQk2HVUY+HChbby4N5777U1s2fPnrWzWnPcE7QA4EZ+B2xOTo60tbVJSUmJtLS0SEpKilRXVw9c+GpqavKasW7YsEFCQkLsn1999ZX8+Mc/tuH6/PPPj+5IACDAhDhB8D7dlGnFxMTYC17R0dFjfToAXKhTIWe4FwEAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwCBFLAVFRWSlJQkUVFRkpGRIfX19Vdtf/78eVmzZo1MnjxZIiMjZfr06XL48OGRnjMABIVwfztUVlZKYWGh7Nq1y4ZreXm5ZGdny5kzZ2TSpElXtO/t7ZUHH3zQfu+NN96QhIQE+fLLL2XixImjNQYACEghjuM4/nQwoTpr1izZsWOH3e/v75fExERZu3atFBUVXdHeBPGf/vQnOX36tIwbN25EJ9nZ2SkxMTHS0dEh0dHRI3oMALjWOePXEoGZjTY0NEhWVtb/HiA01O7X1dX57PPWW29JZmamXSKIi4uTGTNmyObNm6Wvr++Hnz0AuGWJoL293QajCcrLmX0zQ/WlsbFR3nvvPVm6dKlddz179qysXr1aLl68KKWlpT779PT02O3yVxYACDbqVQRmCcGsv7700kuSmpoqOTk5sn79ert0MJSysjI7VfdsZgkCAFwdsLGxsRIWFiatra1ex81+fHy8zz6mcsBUDZh+HnfddZe0tLTYJQdfiouL7TqIZ2tubvbnNAEg+AI2IiLCzkJramq8Zqhm36yz+jJnzhy7LGDaeXz66ac2eM3j+WJKucwi8+UbALh+icCUaO3evVteeeUVOXXqlPzmN7+R7u5uyc/Pt9/Pzc21M1AP8/1vvvlGnnjiCRusVVVV9iKXuegFAG7mdx2sWUNta2uTkpIS+zY/JSVFqqurBy58NTU12coCD7N+euTIESkoKJCZM2faOlgTtuvWrRvdkQBAsNfBjgXqYAG4vg4WADB8BCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwCBFLAVFRWSlJQkUVFRkpGRIfX19cPqd+DAAQkJCZFFixaN5GkBwN0BW1lZKYWFhVJaWirHjx+X5ORkyc7OlnPnzl213xdffCG///3vZe7cuT/kfAHAvQG7fft2WbFiheTn58vdd98tu3btkgkTJsjevXuH7NPX1ydLly6VjRs3ym233fZDzxkA3Bewvb290tDQIFlZWf97gNBQu19XVzdkv+eee04mTZokjz/++LCep6enRzo7O702AHB1wLa3t9vZaFxcnNdxs9/S0uKzz9GjR2XPnj2ye/fuYT9PWVmZxMTEDGyJiYn+nCYAuL+KoKurS5YtW2bDNTY2dtj9iouLpaOjY2Brbm7WPE0AUBHuT2MTkmFhYdLa2up13OzHx8df0f6zzz6zF7cWLlw4cKy/v///nzg8XM6cOSPTpk27ol9kZKTdAOC6mcFGRERIamqq1NTUeAWm2c/MzLyi/Z133iknT56UEydODGwPP/ywzJs3z37NW38AbubXDNYwJVp5eXmSlpYm6enpUl5eLt3d3baqwMjNzZWEhAS7jmrqZGfMmOHVf+LEifbPwccBQK73gM3JyZG2tjYpKSmxF7ZSUlKkurp64MJXU1OTrSwAgOtdiOM4jgQ4U6ZlqgnMBa/o6OixPh0ALtSpkDNMNQFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAAikgK2oqJCkpCSJioqSjIwMqa+vH7Lt7t27Ze7cuXLTTTfZLSsr66rtAeC6DdjKykopLCyU0tJSOX78uCQnJ0t2dracO3fOZ/va2lpZvHixvP/++1JXVyeJiYny0EMPyVdffTUa5w8AASvEcRzHnw5mxjpr1izZsWOH3e/v77ehuXbtWikqKvre/n19fXYma/rn5uYO6zk7OzslJiZGOjo6JDo62p/TBYAxyxm/ZrC9vb3S0NBg3+YPPEBoqN03s9PhuHDhgly8eFFuvvnmIdv09PTYwV6+AUCw8Stg29vb7Qw0Li7O67jZb2lpGdZjrFu3TqZMmeIV0oOVlZXZVxLPZmbIABBsrmkVwZYtW+TAgQNy6NAhe4FsKMXFxXaa7tmam5uv5WkCwKgI96dxbGyshIWFSWtrq9dxsx8fH3/Vvtu2bbMB++6778rMmTOv2jYyMtJuAHDdzGAjIiIkNTVVampqBo6Zi1xmPzMzc8h+W7dulU2bNkl1dbWkpaX9sDMGADfOYA1TopWXl2eDMj09XcrLy6W7u1vy8/Pt901lQEJCgl1HNf74xz9KSUmJ7N+/39bOetZqb7jhBrsBgFv5HbA5OTnS1tZmQ9OEZUpKip2Zei58NTU12coCjxdffNFWHzz66KNej2PqaJ999tnRGAMAuKMOdixQBwvA9XWwAIDhI2ABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAEoIWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAJQQsACghIAFACUELAAoIWABQAkBCwBKCFgAUELAAoASAhYAlBCwAKCEgAUAJQQsACghYAFACQELAIEUsBUVFZKUlCRRUVGSkZEh9fX1V23/+uuvy5133mnb33PPPXL48OGRni8AuDdgKysrpbCwUEpLS+X48eOSnJws2dnZcu7cOZ/tjx07JosXL5bHH39cPv74Y1m0aJHdPvnkk9E4fwAIWCGO4zj+dDAz1lmzZsmOHTvsfn9/vyQmJsratWulqKjoivY5OTnS3d0tb7/99sCxn//855KSkiK7du0a1nN2dnZKTEyMdHR0SHR0tD+nCwBjljPh/jTu7e2VhoYGKS4uHjgWGhoqWVlZUldX57OPOW5mvJczM94333xzyOfp6emxm4cZsOcvAAA0ePLFzznn6AVse3u79PX1SVxcnNdxs3/69GmffVpaWny2N8eHUlZWJhs3brziuJkpA4Cmf//733Yme80D9loxM+TLZ73nz5+XW265RZqamkZt4IH2ymlePJqbm125BOL28V0PY3T7+DzvlKdOnSo333yzjBa/AjY2NlbCwsKktbXV67jZj4+P99nHHPenvREZGWm3wUy4uvWHa5ixMb7g5vYxun18nmXP0eLXI0VEREhqaqrU1NQMHDMXucx+Zmamzz7m+OXtjXfeeWfI9gDgFn4vEZi37nl5eZKWlibp6elSXl5uqwTy8/Pt93NzcyUhIcGuoxpPPPGEPPDAA/LCCy/IggUL5MCBA/LRRx/JSy+9NPqjAYBgDlhTdtXW1iYlJSX2QpUpt6qurh64kGXWSS+fYs+ePVv2798vGzZskKefflp++tOf2gqCGTNmDPs5zXKBqbv1tWzgBowv+Ll9jG4fn9YY/a6DBQAMD/ciAAAlBCwAKCFgAUAJAQsAbg9Yt98C0Z/x7d69W+bOnSs33XST3cy9Hr7v7yPYfn4epmwvJCTE3mEt0Pk7RvMJxDVr1sjkyZPtlenp06cH9O+pv+MzJZp33HGHjB8/3n7Kq6CgQL777jsJRB988IEsXLhQpkyZYn/frnYvFI/a2lq577777M/u9ttvl3379vn/xE4AOHDggBMREeHs3bvX+cc//uGsWLHCmThxotPa2uqz/d///ncnLCzM2bp1q/PPf/7T2bBhgzNu3Djn5MmTTiDyd3xLlixxKioqnI8//tg5deqU89hjjzkxMTHOv/71L8cN4/P4/PPPnYSEBGfu3LnOI4884gQyf8fY09PjpKWlOfPnz3eOHj1qx1pbW+ucOHHCccP4/vrXvzqRkZH2TzO2I0eOOJMnT3YKCgqcQHT48GFn/fr1zsGDB03VlHPo0KGrtm9sbHQmTJjgFBYW2oz585//bDOnurrar+cNiIBNT0931qxZM7Df19fnTJkyxSkrK/PZ/pe//KWzYMECr2MZGRnOr3/9aycQ+Tu+wS5duuTceOONziuvvOK4ZXxmTLNnz3ZefvllJy8vL+AD1t8xvvjii85tt93m9Pb2OsHA3/GZtr/4xS+8jpkwmjNnjhPoZBgB+9RTTzk/+9nPvI7l5OQ42dnZfj3XmC8ReG6BaN4G+3MLxMvbe26BOFT7YBvfYBcuXJCLFy+O6k0oxnp8zz33nEyaNMneiD3QjWSMb731lv04uFkiMB/CMR+s2bx5s70bnRvGZz5AZPp4lhEaGxvt8sf8+fPFDepGKWPG/G5a1+oWiME0vsHWrVtn144G/8CDdXxHjx6VPXv2yIkTJyQYjGSMJnDee+89Wbp0qQ2es2fPyurVq+0Lpfm0ULCPb8mSJbbf/fffb++feunSJVm1apX9tKYbtAyRMeauYt9++61ddx6OMZ/B4uq2bNliLwQdOnTIXnwIdl1dXbJs2TJ7Ic/cnc2tzE2QzAzd3HPD3CDJfMR8/fr1w/5fPAKduQBkZuQ7d+60/3XUwYMHpaqqSjZt2jTWpxZQxnwGe61ugRhM4/PYtm2bDdh3331XZs6cKYHI3/F99tln8sUXX9grupeHkREeHi5nzpyRadOmSbD/DE3lwLhx42w/j7vuusvOjMxbcnNnumAe3zPPPGNfKJcvX273TSWPuenTypUr7QvJaN7ybywMlTHmVo3Dnb0aY/634PZbII5kfMbWrVvtbMDcSMfcuSxQ+Ts+U1p38uRJuzzg2R5++GGZN2+e/ToQ/9eKkfwM58yZY5cFPC8exqeffmqDN5DCdaTjM9cFBoeo58XEDbc3yRytjHECpETElHzs27fPlkSsXLnSloi0tLTY7y9btswpKiryKtMKDw93tm3bZsuYSktLA75My5/xbdmyxZbMvPHGG87XX389sHV1dTluGN9gwVBF4O8Ym5qabOXHb3/7W+fMmTPO22+/7UyaNMn5wx/+4LhhfObfnBnfq6++akua/va3vznTpk2zFT6BqKury5Y9ms3E3vbt2+3XX375pf2+GZsZ4+AyrSeffNJmjCmbDNoyLcPUmU2dOtUGiykZ+fDDDwe+98ADD9h/hJd77bXXnOnTp9v2ppyiqqrKCWT+jO+WW26xvwSDN/NLHaj8/fkFW8COZIzHjh2z5YMmuEzJ1vPPP2/L09wwvosXLzrPPvusDdWoqCgnMTHRWb16tfOf//zHCUTvv/++z39TnjGZP80YB/dJSUmxfx/m5/eXv/zF7+fldoUAoGTM12ABwK0IWABQQsACgBICFgCUELAAoISABQAlBCwAKCFgAUAJAQsASghYAFBCwAKAEgIWAETH/wGUcl2I+0Tk9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f\"{label} Activatiodn Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "926d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "202793f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb7940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetowrk(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "\n",
    "        self.layers = nn.ModuleList([ # 5つの層を実装\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # ショートカットを適用できるかcheck\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20b90ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetowrk(layer_sizes, use_shortcut=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "118d3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target) # outputとtargetがどれくらい近いかに基づいて損失関数を計算\n",
    "\n",
    "    loss.backward() # 勾配を計算\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5dee19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f06586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetowrk(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02948f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/takatorisatoshi/Desktop/playground/tiny-llm/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()  # 現在の作業ディレクトリを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bddf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 1つ上の階層 (project/) を sys.path に追加\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc31ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "from tiny_llm.transormer_block import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "outpout = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", outpout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a17655a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output logits shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from tiny_llm.gpt_model import GPTModel\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"Output logits shape:\", out.shape)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aae33258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0309f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embdding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token embdding layer shape:\",  model.tok_emb.weight.shape)\n",
    "print(f\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2a70cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: -36,237,312\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b6c19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in FeedForward layer: 4,722,432\n",
      "Total number of parameters in Attention layer: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "total_params = sum(p.numel() for p in block.ff.parameters())\n",
    "print(f\"Total number of parameters in FeedForward layer: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Total number of parameters in Attention layer: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d32d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #　合計サイズをバイト数で計算(パラメータひとつあたり4バイトのfloat32と仮定)\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)  \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1c03a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium model total parameters: 406212608\n",
      "Large model total parameters: 838220800\n",
      "XLarge model total parameters: 1637792000\n"
     ]
    }
   ],
   "source": [
    "from tiny_llm.config import GPT_CONFIG_MEDIUM, GPT_CONFIG_LARGE, GPT_CONFIG_XL\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "\n",
    "print(\"Medium model total parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_LARGE)\n",
    "print(\"Large model total parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_XL)\n",
    "print(\"XLarge model total parameters:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1f35d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idxの形状はコンテキストに対応するインデックスの(batch, n_token)配列    \n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # サポートされているコンテキストサイズを超える場合は現在のコンテキストを切り詰める。例えば、LLMがトークンを5つだけサポートしていて、コンテキストのサイズが10の場合は、最後の5つのトークンのみを使用する\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]  # 最後のタイムステップのみに着目し、(batch, n_token, vocab_size)が(batch, vocab_size)になるようにする\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)  # 確率分布に変換 probasの形状は(batch, vocab_size)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # idx_nextの形状は(batch, 1)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # サンプリングしたインデックスを実行中のシーケンスに追加。idxの形状は(batch, n_token+1)になる\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d49f6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # バッチ次元を追加\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c920f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 17775, 22611, 50121, 45688, 36744, 46811]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # 訓練時にのみ使われるドロップアウトのようなランダムなコンポーネントを無効にする\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model= model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66d7400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am minimize firefightersfightsmissing hamstring Townsend\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b544ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "from tiny_llm.config import GPT_CONFIG_124M\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "model.eval() # 訓練時にのみ使われるドロップアウトのようなランダムなコンポーネントを無効にする\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model= model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
